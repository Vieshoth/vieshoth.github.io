---
layout: post
title:  "SPARK SQL"
date:   2019-02-16 19:45:31 +0530
categories: Bigdata
---


SPARK SQL is all about processing structured data

structured data: where the complete structure of data is available. Schema is available. column title, column datatype is all available.

rdbms - mysql, postgres, avro, etc


semi structured data: Complete structure of the data is not available. 

For example: We may have the columns title but without datatype
csv, few nosql, xml, json.

unstructured data:
audio samles, video sample, images, text - 
cannot figure out the data just by giving a sql quiry to unstructured data .
This can be handled by  Machine language - For this sparkML lib cab be used.

spark sql is capable of handling structured data and semi structured data. It can process csv, few nosql, xml, json., mysql, postgres, avro, etc


Components of SPARKSQL.

1. Data source API - Single universal API used for read / write any structured data or semi structured data. 
Also this API has the ablility to discover schema. 
The internal datasource libraries will take care of identifying the schema.

2. Dataframe : DF inherits all the features of RDD. 
- High level representation of data.
- Collection of records + schema => collection of rows + schema => table.

The main diff bw RDD and DF is RDD does not have schema but DF does.
Datasource API will produce DF after reading the datafile.

3. SQL parser and optimizer:
optimizer is called as catalyst.

logical plan -> analysed plan -> catalyst -> optimized plan -> phisical plan

4. Hive thrift service: to make sql lib as sql engine
Hive server is just a interface on top of map reduce so that it can access and process the data in the form of sql. 
but the data is still stored int the HDFS. and it os not stored in the form of database.

and spark developer does not involve in hive thrift service.


Instead of using hive one can use sparksql.
From the application point of view no change has to be made. because hive thrift service will take care of it.


5. Tungsten: It is an internal component and used for memory optimization.
JVM takes care of garbage collection. JVM's allocatio and deallocation of memory is not efficient.
SO spark used tungsten memory management service.
By this memory foot print is reduced by 50%.

Reason behind the DF is faster than RDD is 
1. catalyst - optimizes the query
2. tungsten - optimizes the code.




BI tools are used to communicate with the Hive server
This will act as 



